5/19/25

# Opmizations for LLM

# Decoder only models
# Sampling strategies
# LLM Optmizations

Newer concepts for LLM optimization

1. models are huge
1. 1 trillion parameters
1. latency

Problems
1. cooling

How to reduce the size of the model
1. without loss in the quality

1. KV caching
1. LoRA tuning
1. Distillation
1. Model Pruning
1. Quantization

1. speculative
1. linear attention


